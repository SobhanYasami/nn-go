# Neural Network from Scratch in Go

## ğŸ§  Introduction

This package is written in **Go (Golang)** and implements a **neural network from scratch** â€” without using any external machine learning frameworks.  
Itâ€™s designed to help you understand the mathematical foundations behind:

- **Feed-forward neural networks**
- **Matrix operations**
- **Forward and backward propagation**
- **Gradient descent optimization**

---

## âš™ï¸ How a Multi-Layer Neural Network Works

A neural network consists of **layers of neurons**, each performing a mathematical transformation on the input data.

### ğŸ”¢ Inputs

Let **X** be a batch of input samples with shape **(n Ã— m)**:

- **n** â†’ number of samples
- **m** â†’ number of features per sample

### âš–ï¸ Biases and Weights

Each layer has:

- A **weight matrix** `W` of shape **(k Ã— m)**, where _k_ is the number of neurons in the layer.
- A **bias vector** `B` of shape **(k Ã— 1)**, which shifts the activation values.

### ğŸ” Forward Pass

For the **first layer**:
outputâ‚ = dot(X, Wâ‚áµ€) + Bâ‚

For the **second layer**:
outputâ‚‚ = dot(outputâ‚, Wâ‚‚áµ€) + Bâ‚‚

This process continues for each layer, passing the output of one layer as the input to the next.  
Finally, an **activation function** (such as ReLU or softmax) is applied to introduce non-linearity.

### ğŸ”„ Backward Pass

During **backpropagation**:

- Gradients of the loss with respect to weights and biases are computed.
- Weights are updated using **gradient descent**:

W := W - learning_rate _ dW
B := B - learning_rate _ dB

This allows the network to learn from errors.

---

## ğŸ§© Example Architecture

Input (4 features)
â†“
Dense Layer (3 neurons, ReLU)
â†“
Dense Layer (3 neurons, ReLU)
â†“
Output (Softmax)

Each layer performs a **matrix multiplication**, adds biases, and applies an **activation function**.

---

## ğŸ“š References

- [NumPy Linear Algebra Documentation](https://numpy.org/doc/stable/reference/routines.linalg.html) â€” for understanding matrix operations
- [Matrix Multiplication â€” Wikipedia](https://en.wikipedia.org/wiki/Matrix_multiplication) â€” mathematical foundation of forward pass
- [Golang `math` Package Docs](https://pkg.go.dev/math) â€” Goâ€™s standard math utilities
- [Softmax Function â€” Wikipedia](https://en.wikipedia.org/wiki/Softmax_function) â€” for multi-class classification output
- [Cross-Entropy Loss â€” Wikipedia](https://en.wikipedia.org/wiki/Cross_entropy) â€” loss function used for classification

---

## ğŸš€ Quick Start

```bash
# Clone this repository
git clone https://github.com/SobhanYasami/nn-go.git

cd nn-go

# Run the main example
go run ./cmd
```
